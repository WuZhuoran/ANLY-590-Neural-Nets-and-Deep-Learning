{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "- Here we are going to revisit PCA and introduce some new concepts along the way\n",
    "\n",
    "###  Uses\n",
    "- Feature extraction\n",
    "- Used as input into other models\n",
    "- Visualization of high dimensions in lower spaces\n",
    "- Examples: \n",
    "    - Can use to visualize differences in MNIST $(28\\times 28)$ numbers \n",
    "    - Consider 1080p or 720p images -> millions of pixels\n",
    "    - Genome $3\\times 10^9$base pairs\n",
    "        - Lots of data\n",
    "        - Need to be able to work with high dimensional data\n",
    "- Question How do we go from miliions of sequences to 2 or 3 dimensions?\n",
    "    - Example: DNA sequences from Ancestry or 23AndMe\n",
    "\n",
    "### How PCA works\n",
    "- PCA is a linear transformation:\n",
    "$$\n",
    "Z = Q^Tx\n",
    "$$\n",
    "where $Z$ is the output matrix, $Q^T$ is the affine transformation and $x$ is the input\n",
    "- Matrix $Q$ allows rotation and scaling of $X$\n",
    "- Variance is information\n",
    "    - Directions of high variability indicate most amount of information\n",
    "- PCA results in orthogonal transformation of the input space\n",
    "     - Linear combinatio of inputs\n",
    "     - Arrange columns such that largest variaces in collumns appear first\n",
    "     - Signal to Noise selection\n",
    "         - Large vairations in the data are signal\n",
    "         - Small variations in the data are noise\n",
    "         - Select only columns such that $\\sum Var_i\\approx 95\\%,98\\%,99\\%,...$ of variance\n",
    "\n",
    "<img src=\"./CourseNotes/Lecture 4/pics/pca.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation\n",
    "- We'll derive principal components in 2 steps\n",
    "    1. Calculate the empiricale covariance matrix\n",
    "    2. Decompose the empiricale covariance matrix through eigen decomposition\n",
    "\n",
    "#### Calculating the empirical covariance matrix\n",
    "- Suppose we have the observation matrix $X_{N\\times M}$ then\n",
    "$$\n",
    "\\Sigma_X =\\frac{1}{N}(X-\\mu_X)^T(X-\\mu_X)\\quad;\\quad \\mu_X = \\frac{\\sum_{i=1}^N X_{ij}}{N}\n",
    "$$\n",
    "$\\Sigma_X$ is the empricale covariance matrix and $\\mu_X$ is empircale mean\n",
    "\n",
    "#### Eigendecomposition and selection of Principal components\n",
    "- By definition the eigendecomposition of the empricale covariance matrix is one such that:\n",
    "$$\n",
    "\\Sigma_X V = V \\Lambda \\quad;\\quad\n",
    "\\Lambda = \n",
    "\\begin{bmatrix}\n",
    "\\lambda_1& 0 & \\cdots & 0 \\\\\n",
    "0& \\lambda_2 & \\cdots & 0 \\\\\n",
    "\\vdots&\\vdots&\\ddots&\\vdots\\\\\n",
    "0& 0 & \\ddots & \\lambda_N \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "- By definition $V^T$ is orthogonal to $V$ which implies $V^T=V^{-1}$, which means that by multiplying equation by $V^T$ we have:\n",
    "$$\n",
    "\\Sigma_X = V\\Lambda V^{-1} = V\\Lambda V^T \\Leftrightarrow V^T\\Sigma_XV = \\Lambda\n",
    "$$\n",
    "- Then for some linear transformation $Z = XQ$ we have $\\mu_Z = \\mu_XQ$ which implies:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Sigma_Z &= \\frac{1}{N}(Z-\\mu_Z)^T(Z-\\mu_Z) = \\frac{1}{N}(XQ-\\mu_Z)^T(XQ-\\mu_Z) \\\\\n",
    "& = \\frac{1}{N}(XQ-\\mu_XQ)^T(XQ-\\mu_XQ) = \\frac{1}{N}Q^T(X-\\mu_X)^T(X-\\mu_X)Q\\\\\n",
    "&=Q^T\\Sigma_XQ\n",
    "\\end{align}\n",
    "$$\n",
    "- It follows that if $Q^TQ=I$ then \n",
    "$$\n",
    "\\Sigma_Z = \\Lambda\n",
    "$$\n",
    "and $\\Sigma_Z$ is our principal component matrix\n",
    "\n",
    "#### Reconstruction error\n",
    "- In unsupervised machine learning our objective function is the __reconstruction error__\n",
    "- It is the function against which we \"optimize\" our values\n",
    "- The reconstrcution error for PCA is defined as:\n",
    "$$\n",
    "J = \\sum_n(x_n-x_{reconstructed,n})^2\n",
    "$$\n",
    "- The reconstuction is the loss from using our approximation\n",
    "- Selecting the top $k$ eigenvalues and associated principal vectors we re-write the reconstruction error through the Frobenius norm:\n",
    "$$\n",
    "J = |X- Z_kQ_k^{-1}| = |X- XQ_kQ_k^T|_F^2 \\quad\\mbox{since } Q_k^{-1}=Q_k^T \\mbox{ and } Z_k = XQ_k\n",
    "$$\n",
    "where $X\\in \\mathbb{R}^{N\\times D}$ and $Q\\in \\mathbb{R}^{D\\times K}$\n",
    "- Selecting the eignevectors $Q_i$ associated with the $k$ top principal component yields the principal compnent algorithm, that is \n",
    "$$\n",
    "J_min = \\min_k|X-XQ_kQ_k^T|_F^2\\qquad s.t. k = \\lambda_1>\\lambda_2>...>\\lambda_k\n",
    "$$\n",
    "<img src=\"./CourseNotes/Lecture 4/pics/pca2.gif\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- PCA: Example ----\n",
    "import keras as k\n",
    "import numpy as np\n",
    "from matplotlib.mlab import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of PCA\n",
    "- Only allows linear projections\n",
    "    - Co-variance matrix is of size dxd. If $d=10^5$, then $|\\Sigma|= 10^{10}$\n",
    "    - __Solution__: singular value decomposition (SVD)\n",
    "- PCA restricts to orthogonal vectors in feature space that minimize reconstruction error\n",
    "     - __Solution__: independent component analysis (ICA) seeks directions that are statistically independent\n",
    "- Assumes points are multivariate Gaussian\n",
    "     - __Solution:__ Kernel PCA that transforms input data to other spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of PCA\n",
    "- Two common interpretations of PCA are through\n",
    "    1. Maximum variance direction - find the vectors of the PC such that the projection of the data onto this direction maximizes variances (out of all possible dimensional projections)\n",
    "    2. Minimum reconstruction error - find the PC vectors such that the projection of the data onto these vectors minimizes the reconstruction error\n",
    " \n",
    "<img src=\"./CourseNotes/Lecture 4/pics/reconstruction.png\" />\n",
    "\n",
    "- In both cases PCA provides an understanding of our data in some transformed __latent space__\n",
    "- Frequently these latent spaces don't have an interpretable understanding\n",
    "- Intuitively the idea of linear decomposition of our data is attractive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Eigenfaces Example ####\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Load data\n",
    "lfw_dataset = fetch_lfw_people(min_faces_per_person=100)\n",
    "\n",
    "_, h, w = lfw_dataset.images.shape\n",
    "X = lfw_dataset.data\n",
    "y = lfw_dataset.target\n",
    "target_names = lfw_dataset.target_names\n",
    "\n",
    "# split into a training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Compute a PCA \n",
    "n_components = 100\n",
    "pca = PCA(n_components=n_components, whiten=True).fit(X_train)\n",
    " \n",
    "# Calculate the eignefaces\n",
    "eigenfaces = pca.components_.reshape((n_components, h, w))\n",
    "    \n",
    "# apply PCA transformation\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "def titles(y_test, target_names):\n",
    "    for i in range(target_names.shape[0]):\n",
    "        true_name = target_names[y_test[i]].split(' ')[-1]\n",
    "        yield 'Names:{name}'.format(name = true_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-a7a49f4dca40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0meigenface_titles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"eigenface %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplot_gallery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meigenfaces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meigenface_titles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-c15c745ee806>\u001b[0m in \u001b[0;36mplot_gallery\u001b[1;34m(images, titles, h, w, n_row, n_col)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_row\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "eigenface_titles = [\"eigenface %d\" % i for i in range(0,11)]\n",
    "plot_gallery(eigenfaces, eigenface_titles, h, w)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can take a linear composition of these faces to generate, with 100 components we recreate the images\n",
    "- As an exercise try increasing the number of PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_names = target_names[y_train[0:4]]\n",
    "X_projected_all = pca.inverse_transform(X_train_pca)\n",
    "X_projected = X_projected_all[0:4]\n",
    "loss = ((X_train - X_projected_all) ** 2).mean()\n",
    "plot_gallery(X_projected, reconstructed_names, h, w, n_row=1, n_col=4)\n",
    "print(\"The total training loss is {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True values\n",
    "plot_gallery(X_train[0:12], reconstructed_names, h, w, n_row=1, n_col=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
