{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Honor Statement\n",
    "\n",
    "Name: Zhuoran Wu\n",
    "\n",
    "E-mail: <zw118@georgetown.edu>\n",
    "\n",
    "Platform: Windows\n",
    "\n",
    "In accordance with the class policies and Georgetown's Honor Code,\n",
    "I certify that, with the exceptions of the class resources and those\n",
    "items noted below, I have neither given nor received any assistance\n",
    "on this project.\n",
    "\n",
    "Assignment 0\n",
    "\n",
    "1 Regularization.\n",
    "\n",
    "Using the accompanying Hitters dataset (found here (Links to an external site.)Links to an external site.), we will explore regression models to predict a player's Salary from other variables. You can use any programming languages or frameworks that you wish.\n",
    "\n",
    "1.1 Use LASSO regression to predict Salary from the other numeric predictors (you should omit the categorical predictors).\n",
    "1.1.1. Create a visualization of the coefficient trajectories\n",
    "1.1.2. Comment on which are the final three predictors that remain in the model\n",
    "1.1.3. Use cross-validation to find the optimal value of the regularization penalty\n",
    "1.1.4. How many predictors are left in that model?\n",
    "\n",
    "1.2 Repeat with Ridge Regression.\n",
    "1.2.1 Visualize the coefficient trajectories\n",
    "1.2.2 Use cross-validation to find the optimal value of the regularization penalty\n",
    "\n",
    "2 Short Answer.\n",
    "\n",
    "2.1 Explain in your own words the bias-variance tradeoff.\n",
    "    In Machine Learning Model, We would like to train a model with low bias and low variance. However, in real world,\n",
    "    We could not reach this point. In this case, we need a tradeoff for variance and bias.\n",
    "    In a real model, if we reach a low bias, we will have a high variance, vice versa.\n",
    "    The reason why this tradeoff exists is that we always want to estimate infinite real world data according to\n",
    "    limit data we have. If we trust the limit data we have, we will get a overfitting model with low bias and\n",
    "    high variance. But if we want to trust the prior knowledge of model, we will reach a relative high bias.\n",
    "\n",
    "2.2 What role does regularization play in this tradeoff?\n",
    "\n",
    "    Regularization is a process of introducing additional information\n",
    "    in order to solve an ill posed problem or to prevent overfitting.\n",
    "\n",
    "2.3 Make reference to your findings in number (1) to describe models of high/low bias and variance\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso, LassoCV, Ridge, RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 19960214\n",
    "MAX_ITER = 50000\n",
    "CV_FOLDER = 10\n",
    "TEST_SIZE = 0.2\n",
    "RIDGE_ALPHA = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "hitters = pd.read_csv(\"Hitters.csv\", sep=\",\")\n",
    "\n",
    "# Drop NaN and Name Column\n",
    "hitter_df = hitters.dropna().drop(['Name'], axis=1)\n",
    "\n",
    "# Drop Category Column and get label Salary Column\n",
    "hitter_df = hitter_df.drop(['League', 'Division', 'NewLeague'], axis=1)\n",
    "y = hitter_df['Salary']\n",
    "hitter_df = hitter_df.drop(['Salary'], axis=1)\n",
    "x = hitter_df.astype('float64')\n",
    "\n",
    "# Cross Validation\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression\n",
    "lasso = Lasso(max_iter=MAX_ITER, normalize=True)\n",
    "lasso_coefs = []\n",
    "\n",
    "alphas = 10 ** np.linspace(10, -1, 100) * 0.5\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso.set_params(alpha=alpha)\n",
    "    lasso.fit(scale(x_train), y_train)\n",
    "    lasso_coefs.append(lasso.coef_)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas * 2, lasso_coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.show()\n",
    "\n",
    "lasso_cv = LassoCV(alphas=None, cv=CV_FOLDER, max_iter=MAX_ITER, normalize=True)\n",
    "lasso_cv.fit(x_train, y_train)\n",
    "lasso.set_params(alpha=lasso_cv.alpha_)\n",
    "lasso.fit(x_train, y_train)\n",
    "lasso_mse = mean_squared_error(y_test, lasso.predict(x_test))\n",
    "\n",
    "print(lasso_mse)\n",
    "print(pd.Series(lasso.coef_, index=x.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "\n",
    "ridge = Ridge(normalize=True)\n",
    "ridge_coefs = []\n",
    "for alpha in alphas:\n",
    "    ridge.set_params(alpha=alpha)\n",
    "    ridge.fit(x, y)\n",
    "    ridge_coefs.append(ridge.coef_)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, ridge_coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.show()\n",
    "\n",
    "ridge_alpha = Ridge(alpha=RIDGE_ALPHA, normalize=True)\n",
    "ridge_alpha.fit(x_train, y_train)\n",
    "pred2 = ridge_alpha.predict(x_test)\n",
    "ridge_mse = mean_squared_error(y_test, pred2)\n",
    "\n",
    "print(ridge_mse)\n",
    "print(pd.Series(ridge_alpha.coef_, index=x.columns))\n",
    "\n",
    "ridge_cv = RidgeCV(alphas=alphas, scoring='neg_mean_squared_error', normalize=True)\n",
    "ridge_cv.fit(x_train, y_train)\n",
    "print(ridge_cv.alpha_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
